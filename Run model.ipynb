{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5352620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\" Simple Arabic tokenizer and sentencizer. It is a space-based tokenizer. I use some rules to handle\n",
    "    tokenition exception like words containing the preposition 'و'. For example 'ووالدته' is tokenized to 'و والدته'\n",
    "    :param text: Arabic text to handle\n",
    "    :return: tokenized sentences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = text.decode('utf-8')\n",
    "    except(UnicodeDecodeError, AttributeError):\n",
    "        pass\n",
    "    text = text.strip()\n",
    "    tokenizer_exceptions = [\"وظف\", \"وضعها\", \"وضعه\", \"وقفنا\", \"وصفوها\", \"وجهوا\", \"والدته\", \"والده\", \"وادي\", \"وضعية\",\n",
    "                            \"واجهات\", \"وفرتها\", \"وقاية\", \"وفا\", \"وزيرنا\", \"وزارتي\", \"وجهاها\", \"واردة\", \"وضعته\",\n",
    "                            \"وضعتها\", \"وجاهة\", \"وهمية\", \"واجهة\", \"واضعاً\", \"واقعي\", \"ودائع\", \"واعدا\", \"واع\", \"واسعا\",\n",
    "                            \"ورائها\", \"وحدها\", \"وزارتي\", \"وزارتي\", \"والدة\", \"وزرائها\", \"وسطاء\", \"وليامز\", \"وافق\",\n",
    "                            \"والدها\", \"وسم\", \"وافق\", \"وجهها\", \"واسعة\", \"واسع\", \"وزنها\", \"وزنه\",\n",
    "                            \"وصلوا\", \"والدها\", \"وصولاً\", \"وضوحاً\", \"وجّهته\", \"وضعته\", \"ويكيليكس\", \"وحدها\", \"وزيراً\",\n",
    "                            \"وقفات\", \"وعر\", \"واقيًا\", \"وقوف\", \"وصولهم\", \"وارسو\", \"واجهت\", \"وقائية\", \"وضعهم\",\n",
    "                            \"وسطاء\", \"وظيفته\", \"ورائه\", \"واسع\", \"ورط\", \"وظفت\", \"وقوف\", \"وافقت\", \"وفدًا\", \"وصلتها\",\n",
    "                            \"وثائقي\", \"ويليان\", \"وساط\", \"وُقّع\", \"وَقّع\", \"وخيمة\", \"ويست\", \"والتر\", \"وهران\", \"ولاعة\",\n",
    "                            \"ولايت\", \"والي\", \"واجب\", \"وظيفتها\", \"ولايات\", \"واشنطن\", \"واصف\",\n",
    "                            \"وقح\", \"وعد\", \"وقود\", \"وزن\", \"وقوع\", \"ورشة\", \"وقائع\", \"وتيرة\", \"وساطة\", \"وفود\", \"وفات\",\n",
    "                            \"وصاية\", \"وشيك\", \"وثائق\", \"وطنية\", \"وجهات\", \"وجهت\", \"وعود\", \"وضعهم\", \"وون\", \"وسعها\", \"وسعه\",\n",
    "                            \"ولاية\", \"واصفاً\", \"واصلت\", \"وليان\", \"وجدتها\", \"وجدته\", \"وديتي\", \"وطأت\", \"وطأ\", \"وعودها\",\n",
    "                            \"وجوه\", \"وضوح\", \"وجيز\", \"ورثنا\", \"ورث\", \"واقع\", \"وهم\", \"واسعاً\", \"وراثية\", \"وراثي\", \"والاس\",\n",
    "                            \"واجهنا\", \"وابل\", \"ويكيميديا\", \"واضحا\", \"واضح\", \"وصفته\", \"واتساب\", \"وحدات\", \"ون\",\n",
    "                            \"وورلد\", \"والد\", \"وكلاء\", \"وتر\", \"وثيق\", \"وكالة\", \"وكالات\", \"و احدة\", \"واحد\", \"وصيته\",\n",
    "                            \"وصيه\", \"ويلمينغتون\", \"ولد\", \"وزر\", \"وعي\", \"وفد\", \"وصول\", \"وقف\", \"وفاة\", \"ووتش\", \"وسط\",\n",
    "                            \"وزراء\", \"وزارة\", \"ودي\", \"وصيف\", \"ويمبلدون\", \"وست\", \"وهج\", \"والد\", \"وليد\", \"وثار\",\n",
    "                            \"وجد\", \"وجه\", \"وقت\", \"ويلز\", \"وجود\", \"وجيه\", \"وحد\", \"وحيد\", \"ودا\", \"وداد\", \"ودرو\",\n",
    "                            \"ودى\", \"وديع\", \"وراء\", \"ورانس\", \"ورث\", \"ورَّث\", \"ورد\", \"وردة\", \"ورق\", \"ورم\", \"وزير\",\n",
    "                            \"وسام\", \"وسائل\", \"وستون\", \"وسط\", \"وسن\", \"وسيط\", \"وسيلة\", \"وسيم\", \"وصاف\", \"وصف\", \"وصْفَ\",\n",
    "                            \"وصل\", \"وضع\", \"وطن\", \"وعاء\", \"وفاء\", \"وفق\", \"وفيق\", \"وقت\", \"وقع\", \"وكال\", \"وكيل\",\n",
    "                            \"ولاء\", \"ولف\", \"وهب\", \"وباء\", \"ونستون\", \"وضح\", \"وجب\", \"وقّع\", \"ولنغتون\", \"وحش\",\n",
    "                            \"وفر\", \"ولادة\", \"ولي\", \"وفيات\", \"وزار\", \"وجّه\", \"وهماً\", \"وجَّه\", \"وظيفة\", \"وظائف\", \"وقائي\"]\n",
    "\n",
    "    sentence_splitter_exceptions = [\"د.\", \"كي.\", \"في.\", \"آر.\", \"بى.\", \"جى.\", \"دى.\", \"جيه.\", \"ان.\", \"ال.\", \"سى.\", \"اس.\",\n",
    "                                    \"اتش.\", \"اف.\"]\n",
    "\n",
    "    sentence_splitters = ['.', '!', '؟', '\\n']\n",
    "    text = text.replace('،', ' ، ')\n",
    "    text = text.replace('*', ' * ')\n",
    "    text = text.replace('’', ' ’ ')\n",
    "    text = text.replace('‘', ' ‘ ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    text = text.replace('(', ' ( ')\n",
    "    text = text.replace(')', ' ) ')\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('[', ' [ ')\n",
    "    text = text.replace(']', ' ] ')\n",
    "    text = text.replace('|', ' | ')\n",
    "    text = text.replace('؛', ' ؛ ')\n",
    "    text = text.replace('«', ' « ')\n",
    "    text = text.replace('»', ' » ')\n",
    "    text = text.replace('!', ' ! ')\n",
    "    text = text.replace('-', ' - ')\n",
    "    text = text.replace('“', ' “ ')\n",
    "    text = text.replace('”', ' ” ')\n",
    "    text = text.replace('\"', ' \" ')\n",
    "    text = text.replace('؟', ' ؟ ')\n",
    "    text = text.replace(':', ' : ')\n",
    "    text = text.replace('…', ' … ')\n",
    "    text = text.replace('..', ' .. ')\n",
    "    text = text.replace('...', ' ... ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.replace('\\n', ' \\n ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token[-1] in sentence_splitters:\n",
    "            is_exceptions = token in sentence_splitter_exceptions\n",
    "            if not is_exceptions:\n",
    "                tokens[i] = token[:-1] + ' ' + token[-1] + 'SENT_SPLITTER'\n",
    "    tokens = ' '.join(tokens).split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith('و'):\n",
    "            is_exceptions = [token.startswith(exception) and len(token) <= len(exception) + 1 for exception in\n",
    "                             tokenizer_exceptions]\n",
    "            if True not in is_exceptions:\n",
    "                tokens[i] = token[0] + ' ' + token[1:]\n",
    "    text = (' '.join(tokens))\n",
    "    text = text.replace(' وال', ' و ال')\n",
    "    text = text.replace(' لل', ' ل ل')\n",
    "    text = text.replace(' لإ', ' ل إ')\n",
    "    text = text.replace(' بالأ', ' ب الأ')\n",
    "    text = text.replace('وفقا ل', 'وفقا ل ')\n",
    "    text = text.replace('نسبة ل', 'نسبة ل ')\n",
    "\n",
    "#add by me \n",
    "    text = text.replace(' بال', ' ب ال')\n",
    "    text = text.replace(' بب', ' ب ب')\n",
    "\n",
    "\n",
    "    #sentences = text.split('SENT_SPLITTER')\n",
    "   # return sentences\n",
    "    return text\n",
    "\n",
    "#clean word from dump chars:\n",
    "\n",
    "dump_chars = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~’،ـ؟؛«» '\n",
    "def clean_word(word):\n",
    "    word = word.translate(str.maketrans({key: None for key in dump_chars})) \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    word = re.sub(p_tashkeel,\"\", word)\n",
    "    \n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff000ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "جده B-LOC\n",
      "التواضع M\n",
      "الحياء M\n",
      "العامل B-LOC\n",
      "الحبشة B-LOC\n",
      "الشام B-LOC\n",
      "التواضع M\n",
      "رحابة M\n",
      "رحابة M\n",
      "الشام B-LOC\n",
      "الروم B-LOC\n",
      "الشام B-LOC\n",
      "الفرات B-LOC\n",
      "آسيا B-LOC\n",
      "الشام B-LOC\n",
      "الأردن B-LOC\n",
      "بيسان B-LOC\n",
      "locations :  {'العامل', 'الروم', 'الحبشة', 'بيسان', 'آسيا', 'الشام', 'الأردن', 'الفرات', 'جده'} manners :  {'رحابة', 'الحياء', 'التواضع'}\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# load the model from disk\n",
    "modelfilename = r'C:\\my lap\\Graduation proj\\ArabicNER\\Predication\\lsvm.sav'\n",
    "loaded_model = pickle.load(open(modelfilename, 'rb'))\n",
    "\n",
    "# load the vectorizer from disk\n",
    "vectorizerfilename = r'C:\\my lap\\Graduation proj\\ArabicNER\\Predication\\vectorizer.pickle'\n",
    "loaded_vectorizer = pickle.load(open(vectorizerfilename, 'rb'))\n",
    "\n",
    "# load the tfidf from disk\n",
    "tfidffilename = r'C:\\my lap\\Graduation proj\\ArabicNER\\Predication\\tfidf.pickle'\n",
    "loaded_tfidf = pickle.load(open(tfidffilename, 'rb'))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def RunNER(phrase):\n",
    "    phrase=preprocess(phrase)\n",
    "    arr = phrase.split()\n",
    "    y = []\n",
    "    token = []\n",
    "    for x in arr:\n",
    "        x=clean_word(x)\n",
    "        x = [x]\n",
    "        test_str = loaded_vectorizer.transform(x)\n",
    "        test_tfstr = loaded_tfidf.transform(test_str)\n",
    "        test_tfstr.shape\n",
    "        token.append(x)\n",
    "        y.append(loaded_model.predict(test_tfstr.toarray())[0])\n",
    "    return token, y\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "f = open(r\"C:\\my lap\\Graduation proj\\ArabicNER\\Predication\\Abo.txt\", 'r', encoding='utf8')\n",
    "phrase = str(f.read())\n",
    "token, y = RunNER(phrase)\n",
    "\n",
    "\n",
    "# for phrase in read_file:\n",
    "#     arr=phrase.split()\n",
    "# phrase=\"هو مؤسس التقويم الهجري، وفي عهده بلغ الإسلام مبلغًا عظيمًا، وتوسع نطاق الدولة الإسلامية حتى شمل كامل العراق ومصر وليبيا والشام وفارس وخراسان وشرق الأناضول وجنوب أرمينية وسجستان،\"\n",
    "\n",
    "\n",
    "# print Results\n",
    "l = set()\n",
    "m = set()\n",
    "for i in range(0, len(token)):\n",
    "    if (y[i] == \"B-LOC\"):\n",
    "        l.add(token[i][0])\n",
    "        print(token[i][0], y[i])\n",
    "    elif(y[i] == \"M\"):\n",
    "        m.add(token[i][0])\n",
    "        print(token[i][0], y[i])\n",
    "print(\"locations : \",l,\"manners : \", m)\n",
    "\n",
    "\n",
    "# df=pd.DataFrame(list(zip(token,y)),columns=['token','entity_type'])\n",
    "# df=pd.DataFrame(list(zip(token,y)),columns=['الكلمه ','نوعها '])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b46409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فبلغ ا شرقا و آسيا الصغرى شمالا قدم أمير المؤمنين عمر الشام فتلقاه ، و يقال إنه من مهاجرى الحبشة ، شهد بدرا فتح أبوعبيدة بن الجراح ب الشام كان وفاة أبو عبيدة بن الجراح و هو ابن ثمان و خمسين سنة ب الأردن و قُبر ب بيسان\n"
     ]
    }
   ],
   "source": [
    "print(preprocess( '''  فبلغ ا\n",
    "شرقا وآسيا الصغرى شمالا  قدم أمير المؤمنين عمر الشام فتلقاه ، ويقال إنه من مهاجرى الحبشة ، شهد بدرا فتح أبوعبيدة بن الجراح بالشام كان وفاة أبو عبيدة بن الجراح وهو ابن ثمان وخمسين  سنة بالأردن  وقُبر ببيسان  '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea9c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
